{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "984213ef-8da4-4388-91c0-42806185f21c",
   "metadata": {},
   "source": [
    "# 1. The architecture of modern GPU\n",
    "* GPUs are organized into an array of highly threaded Streaming Multiprocessors(SMs).\n",
    "* Each SM has many cores, known as CUDA cores.\n",
    "* Each SM has an on-chip memory structure. \n",
    "* GPUs also come with a large amount of off-chip memory called \"Global Memory.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fa3e29-88f1-4916-88c7-1dffca03aca5",
   "metadata": {},
   "source": [
    "# 2. Block scheduling\n",
    "* Threads are assigned to SMs on a block-by-block basis. All the threads in a block are assigned to the same SM simultaneously.\n",
    "* Blocks must reserve hardware resources before being assigned to an SM, so only a limited number of blocks can be assigned simultaneously. \n",
    "* Each GPU has a limited number of SMs, each of which can be assigned a limited number of blocks simultaneously. Hence, there is a limit to how many blocks can simultaneously run on a CUDA device. \n",
    "* The runtime maintains a list of blocks to be executed and assigns new blocks to SMs when previously assigned blocks have finished. \n",
    "* Block-by-block thread assignment allows for special interactions among threads in the same block that are impossible among threads of different blocks. This includes barrier sync and low-latency access to shared memory. \n",
    "* Threads in different blocks can also synchronize if they follow certain patterns, but that is outside the scope of this chapter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bfac15-be62-4d55-a48b-8c8a4b07fec9",
   "metadata": {},
   "source": [
    "# 3. Synchronization and transparent scalability\n",
    "* __synchthreads() allows threads in the same block to coordinate. When a thread calls __syncthreads, it will be held at that location until all threads in the same block have reached that location. This kind of synchronization is called **Barrier Synchronization**.\n",
    "* If in a kernel __synchthreads() is present, all threads must execute it, or none should. Incorrect usage of __synchthreads will result in undefined behavior.\n",
    "* Barrier Synchronization enforces strict constraints on threads within a block:\n",
    "    * All threads in a block should execute near each other to avoid excessive long time.\n",
    "    * The system must ensure all threads participating in Barrier Synchronization have the necessary resources.\n",
    "CUDA achieves these constraints by enforcing block-by-block scheduling.\n",
    "* CUDA hits a significant trade-off by not allowing Barrier Synchronization across blocks. Blocks can be executed in any order and independent of each other. Independent block execution also provides for different hardware with different power/performance/cost profits. Albeit at various speeds. This is called **Transparent Scalability**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da501bf7-04fc-4fae-8a94-c49d4c018402",
   "metadata": {},
   "source": [
    "# 4. Warps and SIMD hardware\n",
    "* Threads in a block execute independently of each other and only follow Barrier Synchronization rules.\n",
    "* Thread scheduling in CUDA is hardware-specific.\n",
    "* In most implementations, once a block is assigned to an SM, the threads inside the block are divided into 32 thread units called warps. Knowledge of warps can be crucial in optimizing performance on a CUDA device.\n",
    "* Blocks are partitioned into warps based on thread indices. For multi-dimensional blocks, the threads are laid in a row-major format and then split into warps of 32. After the threads have been ***linearized***, the split into blocks is done as follows:\n",
    "$$\n",
    "\\text{warp}_n = [32n, 32(n - 1) + 1]\n",
    "$$\n",
    "The last warp is padded with inactive threads if required.\n",
    "* Higher coordinates are laid before lower coordinates to do the row-major **linearization**. Concretely, z, then y, and then x. Refer to Figure 4.1.\n",
    "* Execution units (cores) in an SM are grouped into **processing blocks**. All cores in the same processing block share the same Instruction Fetch/Dispatch units. For example, A100 has 64 cores divided into four processing blocks of 16 cores.\n",
    "* All threads in the same warp are assigned to the same processing block. They apply the same instruction to different parts of the data at the same time. This model is called Single Instruction Multiple Data(SIMD). The advantage of SIMD is that the same control structure is shared across many compute units. Hence, large amounts of hardware can be assigned to compute cores. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8fe9ac-2082-4d7a-b4c5-0fab59912ba2",
   "metadata": {},
   "source": [
    "# 5. Control divergence\n",
    "* When different threads in the same warp take different control flow paths, the SIMD hardware will take multiple passes through these paths, one for each path.\n",
    "* When different threads in the same warps execute different paths, these threads exhibit **control divergence**.\n",
    "* The multipass approach to divergent warp execution extends the SIMD hardware capability to implement the full semantics of CUDA threads.\n",
    "* While the SIMD hardware executes the same instructions for all threads in the warp, it selectively lets the thread take part in the paths that they took.\n",
    "* The multipass approach preserves thread independence, allowing us to continue taking advantage of low-cost SIMD hardware. The trade-off is extra passes on the same instruction set.\n",
    "* After Pascal architecture, many passes can be executed concurrently. This feature is called **Independent Thread Scheduling**.\n",
    "* Divergence can also arise from loops.\n",
    "* A prevalent reason for control divergence is to handle boundary conditions. As such, the larger the compute load, the lesser the impact of control divergence. This is because the larger the grid size, the smaller the percentage of warps involved in boundary condition paths.\n",
    "* Another impact of control divergence is that one cannot assume the same execution time for all threads. Therefore, if all threads in a warp must complete a phase before moving on, we must use a barrier synchronization mechanism, like __synchwarps()."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
